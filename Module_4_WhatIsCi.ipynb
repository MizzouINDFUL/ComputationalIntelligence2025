{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did you sign up for?\n",
    "\n",
    "This page is **NOT** an indepth technical analysis of Computational Intelligence\n",
    "\n",
    "That is what this class is :)\n",
    "\n",
    "This page is an    \n",
    "\n",
    "**I N T R O D U C T I O N**\n",
    "\n",
    "An *official* definition from our society is https://cis.ieee.org/about/what-is-ci\n",
    "\n",
    "  * \"<b>Computational Intelligence (CI) is the theory, design, application and development of biologically and linguistically motivated computational paradigms</b>. Traditionally the three main pillars of CI have been Neural Networks, Fuzzy Systems and Evolutionary Computation. However, in time many <b>nature inspired computing paradigms have evolved</b>. Thus CI is an evolving field and at present in addition to the three main constituents, it encompasses computing paradigms like ambient intelligence, artificial life, cultural learning, artificial endocrine networks, social reasoning, and artificial hormone networks. CI plays a major role in developing successful intelligent systems, including games and cognitive developmental systems. Over the last few years there has been an explosion of research on Deep Learning, in particular deep convolutional neural networks. Nowadays, deep learning has become the core method for artificial intelligence. In fact, some of the most successful AI systems are based on CI.\"\n",
    "\n",
    "Lets take a *little more focused peek* at the three most well-known areas of Computational Intelligence\n",
    "\n",
    "* Neural networks\n",
    "* Fuzzy sets \n",
    "* Evolutionary algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Neural Networks in 5 mins ...\n",
    "\n",
    "You can get a more indepth description at \n",
    "\n",
    "* Our text book\n",
    "* Haykin's neural network book: https://www.amazon.com/Neural-Networks-Learning-Machines-3rd/dp/0131471392\n",
    "* Deep learning FREE book: https://www.deeplearningbook.org\n",
    "* Online convolutional neural network site: http://cs231n.github.io/neural-networks-3/\n",
    "* Our (EECS at Mizzou) Neural Network class\n",
    "* MANY MANY MANY other web resources!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a neural network is *motivated by the idea* of a neuron and network of neurons in the human brain\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png\">\n",
    "\n",
    "Neural networks can do a lot of things, e.g., help us *store* information and perform *computation*. \n",
    "\n",
    "A single neuron is nice and all, but the \"network\" in **neural networks** implies that a neuron is a basic building block (below, each circle is a neuron shown above)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\">\n",
    "\n",
    "note: you might enjoy reading https://en.wikipedia.org/wiki/Universal_approximation_theorem, a theorem (existance theorem, it does not tell us how to find such a network) that informs us about the space of functions that particular types of neural networks can compute\n",
    "\n",
    "Below is a simple little animation that highlights data-driven learning (most neural networks are trained using data, e.g., a set of images of cats!) of a multi-layer perceptron (a type of neuron): specifically it is a two class (red and blue points) problem (e.g., imagine one class represents \"dogs\" and the other represents \"cats\")\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*Ny6h0V70Ve_hFgBAFKiMJg.gif\">\n",
    "\n",
    "That animation shows you the data points (e.g., features we extract from images of cats and dogs) and the neural network is a *machine* (in this case a two class nonlinear function that encodes a decision boundary) and the animation shows a step-by-step learning of the machine from the training data.\n",
    "\n",
    "Well, enough already, neurons and neural networks have been around for a LONG TIME (since the 1940s!!!). So, what made neural nets hot again? A lot of things (we will discuss some when we go into that module). \n",
    "\n",
    "Just to give you a sneak peek, data and computing are two of the **biggest contributors** vs recent theoretical (e.g., math) breakthroughs. Furthermore, computer vision helped spawn our NN resurgence. Even furthermore, everyone loves to talk about deep learning using networks like convolutional neural nets (CNN)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg\">\n",
    "\n",
    "In the above picture, a CNN is a multi-layer neural structure that we usually equate with *learning how to extract features* (edges, colors, eyes, faces, etc.) followed by *reasoning* about those components. More to come later! Here is an example of a CNN called YOLO (You Only Look Once) for real-time object detection (can you find me a cat in an image?) and localization (where in the image is the cat?).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2560/0*HMacEfECt2PYQOxF.jpg\">\n",
    "<img src=\"https://miro.medium.com/max/2000/1*d4Eg17IVJ0L41e7CTWLLSg.png\">\n",
    "\n",
    "And yes, you are all likely aware of the family of recurrent NNs and large language models (LLMs), powered by tools like transformers\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"50%\">\n",
    "\n",
    "**BUT** there is **MUCH** more to neural networks than deep learning, e.g., self-supervised learning, unsupervised learning with networks like self organizing maps (SOM) or neural gas, long short term memory (LSTM), reinforcement learning, graph neural networks, and a lot (lot! (lot!)) more. The following animation is a simple depiction of a simple unsupervised network (a SOM) that fits neurons to data (later we can use fancy words like manifold learning in high dimensional spaces for quantization or reduction or blah blah blah)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Somtraining.svg/500px-Somtraining.svg.png\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/TrainSOM.gif/440px-TrainSOM.gif\">\n",
    "\n",
    "Like I said, this is neural nets in *a few* minutes. We will go into depth in the NN class module. Oh, I should mention, neural networks are being used for a lot of stuff right now like\n",
    "\n",
    " * natural language processing\n",
    " * computer vision\n",
    " * autonomy and decision making\n",
    " * bioinformatics\n",
    " * remote sensing\n",
    " * material design\n",
    " * geeze, a lot (lot (lot (...))) more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Fuzzy Sets in 5 mins ...\n",
    "\n",
    "OK, hopefully I sparked your interest with the above, now its time to move onto the next topic !!!!\n",
    "\n",
    "In general, fuzzy sets are a precise mathematics for imprecise data/information :)\n",
    "\n",
    "**What does that mean?** \n",
    "\n",
    "You have heard about probability theory and statistics surely. Turns out, probability/stats do not address all *types* of uncertainty, data/information, and applications. In this module, we will focus on math to <b>model</b> uncertainty and <b>compute</b> on uncertain data/information based on an idea coined \"fuzzy sets\" by Lotfi Zadeh. \n",
    "\n",
    "Lets start with \"what is a fuzzy set\"? \n",
    "\n",
    "A good and intuitive example is the concept of a *linguistic variable*.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Amy_Obrien2/publication/36711246/figure/fig1/AS:669442602184718@1536618962885/5-Human-age-groups-illustrated-as-fuzzy-sets-and-membership-functions-Young.png\">\n",
    "\n",
    "The x-axis is age and the y-axis is the *degree of membership*. The variables here is AGE. The attributes are Young, Middle-Aged, and Old. So, AGE = {Young, Middle-Aged, Old}. At each x-axis location, the y-axis is the *degree to which that age is one of our attributes*. So, what is $\\mu_{Old}(20)$ (where $\\mu_{Old} \\in [0,1]$ is our membership function and 20 is the age under question)? Is there a clear age where you are old? :) When you turn 50 does that mean you are now old? I hope not! If I asked each of you, do you think you would agree... We will get into this more later, but, there are a lot of questions we need to address from how do we model these concepts to how do we use this information and what is the *nature* of the underlying uncertainity (possibility, belief, subjective probability, relative frequency of occurrence, etc.). \n",
    "\n",
    "Next, lets talk about *logic*, specifically approximate reasoning using fuzzy IF-THEN rules. This is one robust way to build an \"expert system\" for tasks like control theory, robotics, computer vision, and a BUNCH of other cool jazz.\n",
    "\n",
    "(Note, if you like to think about the \"scale spectrum\", say (no data)--to--(tons of data), NNs have debatably worked best on the 'right side' and many fuzzy logic systems to date have shown success on the 'left side'. However, there is nothing that prohibits one or the other from working across the spectrum. There are fuzzy logic learning algorithms and clearly people are interested in building neural networks on 'small data')\n",
    "\n",
    "<img src=\"https://www.mathworks.com/help/fuzzy/sugeno_const_tipping.png\">\n",
    "\n",
    "In the above, there are three rules, the first of whcih is IF the service is poor OR the food is rancid THEN the tip is cheap. You can see sets that represent these concepts and each input (shown vertically) gets its membership degree determined from the sets in these rules and horizontally these values are passed along (we will look at how) to determine the membership in the output set (tip). If the rules are on the same domain, e.g., \"tip\", then their results need to be combined and if you waiter does not like fuzzy sets they will expect a crisp numeric amount (e.g., 20 cents...), so we have to perform a task like \"defuzzification\" (crisp output from uncertain information). We will talk about all of this in the following weeks.\n",
    "\n",
    "**Next, lets chat about *unsupervised learning*.** That is, methods to learn from data (e.g., discover patterns) when we have no knowledge about the data (e.g., labels). In the neural network section above we talked about a supervised learning aided *classifier* to recognize cats vs dogs. For unsupervised, imaging have a folder of images with no labels. We have data, but what? In this class, we will look at an algorithm called fuzzy clustering (the fuzzy c-means). Here is a cool pic showing just a few of the MANY clustering algorithms that exist nowadays. The columns are algorithms and the rows are data sets. You can see that different algorithms produce different *results*.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\">\n",
    "\n",
    "Oh, there is a lot more to fuzzy set theory than just the above, e.g., computing with words (vs a paradigm that *computes with numbers*)\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/6ddd51ae12b51b6a3f60ab6d15d68b6d047a571e/13-Figure1.8-1.png\">\n",
    "\n",
    "In this class, we will discuss *fuzzy fusion/aggregation*. Wish I had a pretty picture for fusion. Instead, I will show you a picture of the guy (he currently runs a research center at MTU) that I have worked on fusion with for a decade (he shared the photo with me so I need to get some mileage out of it):\n",
    "\n",
    "<img src=\"https://blogs.mtu.edu/icc/files/2019/06/Timothy-Havens-150dpi.jpg\"> \n",
    "\n",
    "You can find many of our publications at http://derektanderson.com/pubs.html.\n",
    "\n",
    "In the Management Under Uncertainty graduate student class we will talk more about fuzzy aggregation/fusion and its applications to tasks like signal/image processing and computer vision, regression, multi-criteria decision making, and much more!!! \n",
    "\n",
    "**The point is, this module will focus on non-statistical (you get that in your stats and related classes here at Mizzou) uncertainty in AI/CI**\n",
    "\n",
    "OK, you argumentive guys/gals, is this section of the class \"against stats\"? NO!!! Statistics is wonderful and it has found its way into some of our deepest parts of artificial intelligence, physics, the universe, etc. Maybe you had a programming teacher and you asked them, \"what is the best programming language\" and you got a response like, \"different problems require different tools\". IF we have time (see what I did, that's a rule!), we might even take a peek at something more recent like:\n",
    "\n",
    "* Do a search for Bart Kosko and rule foam. He has been working to make connections and mappings across NNs and FL (and even uses probability theory!)\n",
    "    * <a href=\"https://ieeexplore.ieee.org/document/9494525\">Bayesian Pruned Random Rule Foams for XAI\n",
    "</a>\n",
    "    * <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-81561-5_21\">Random Fuzzy-Rule Foams for Explainable AI</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Evolutionary Computation in 5 mins ...\n",
    "\n",
    "OK, so NNs give us a way to store and compute based (well, motivated by) on biological processes (i.e., neural networks in the brain!). Next, fuzzy sets give us a way to mathematically model uncertainity and do human-centric things like logic and computing with words. We have talked a lot about modeling and computing, but what about **LEARNING**? \n",
    "\n",
    "In this module we will discuss *classical* methods of learning, e.g., Calculus and gradient descent. However, when problems become intractible (computationally and/or analytically), we often resort to *alternative optimization methods*; enter evolutionary computation. In general, we can think of this topic as learning inspired by nature, for example:\n",
    "\n",
    " * Genetic algorithms\n",
    " * Ant colony optimization\n",
    " * Particle swarm optimization\n",
    "\n",
    "First, lets talk about local and global maxima\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyVw7-Nu5hJs5ekiqhr3_CKzJgbDJlquIH8kUrOp5BeqAOTdMaKw\">\n",
    "\n",
    "And hill climbing \n",
    "\n",
    "<img src=\"http://yingzhenli.net/home/en/wp-content/uploads/2015/11/McCormick.gif\">\n",
    "\n",
    "The first technique we will look at in this module is a genetic algorithm\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Xia_Ting_Feng/publication/245282272/figure/fig2/AS:441843335864321@1482355068268/Crossover-and-mutation-operations-in-genetic-algorithm.png\">\n",
    "\n",
    "What you need to know today is, genetic algorithms have chromosomes (our *workers*) and they work together/communicate to solve our problem! More on that later.\n",
    "\n",
    "<img src=\"https://pablormier.github.io/assets/img/de/ackley.gif\">\n",
    "\n",
    "Here is a cute little animation showing a population of different evolved strategies :) Just fun to watch. \n",
    "\n",
    "<img src=\"https://i1.wp.com/blog.datascienceheroes.com/content/images/2019/01/evolutionary_algortihm-1.gif?w=456&ssl=1\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
